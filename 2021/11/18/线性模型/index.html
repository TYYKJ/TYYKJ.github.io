<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="线性模型"><meta name="keywords" content="math,torch"><meta name="author" content="励图研发团队,undefined"><meta name="copyright" content="励图研发团队"><title>线性模型 | 励图研发团队</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@latest/dist/reveal.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@latest/dist/reset.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@latest/dist/theme/black.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@latest/plugin/highlight/monokai.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="励图研发团队" type="application/atom+xml">
</head><body><div class="reveal"><div class="slides"><section data-markdown data-separator="---" data-separator-vertical="--" data-charset="utf-8"><script type="text/template">
## 线性模型

为了解释*线性回归*，我们举一个实际的例子：我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。这个数据集包括了房屋的销售价格、面积和房龄。在机器学习的术语中，该数据集称为*训练数据集*（training data set）或*训练集*（training set），每行数据（在这个例子中是与一次房屋交易相对应的数据）称为*样本*（sample），也可以称为*数据点*（data point）或*数据样本*（data instance）。我们要试图预测的目标（在这个例子中是房屋价格）称为*标签*（label）或*目标*（target）。预测所依据的自变量（面积和房龄）称为*特征*（feature）或*协变量*（covariate）。

通常，我们使用 $n$ 来表示数据集中的样本数。对索引为$i$的样本，其输入表示为$\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\top$，其对应的标签是$y^{(i)}$。

线性假设是指目标（价格）可以表示为特征（面积和房龄的加权和），如下面的式子：
$$
\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.
$$

<p align="center">公式1.1</p>

1.1中的$w_{\mathrm{area}}$成为权重，$b$称为偏置。权重决定了每个特征对于预测值的影响。偏置是当所有特征都取0时，预测值应该是多少。即使现实情况中不存在任何房子面子和房龄正好为0年，仍然需要偏置项，如果没有偏置项，模型的表达能力将受到限制。严格来说1.1是输入特征的仿射变换。仿射变换的特点是通过加权和对特征进行线性变换，并通过偏置项进行平移。

给定一个具体的数据，我们需要做的是寻找模型的权重$w$和偏置$b$，使得根据模型做出的预测结果要大致符合数据里的真实价格。输出的预测值由输入特征通过**线性变换**的仿射变换决定，仿射变换由所选权重和偏置确定。

在机器学习领域，特征往往是高维数据，建模时通常采用线性代数表示法。当输入包含$d$个特征时，我们将预测结果$\hat{y}$表示为：
$$
\hat{y} = w_1  x_1 + ... + w_d  x_d + b.
$$

<p align="center">公式1.2</p>

将所有特征放入向量$\mathbf{x} \in \mathbb{R}^d$，并将所有权重放到$\mathbf{w} \in \mathbb{R}^d$中，可以使用点积来简洁的表达模型：
$$
\hat{y} = \mathbf{w}^\top \mathbf{x} + b.
$$

<p align="center">公式1.3</p>

在1.3中，向量$\mathbf{x}$对应与单个数据样本的特征。用符号表示的矩阵$\mathbf{X} \in \mathbb{R}^{n \times d}$可以方便的引用整个数据集的$n$个样本。其中，$\mathbf{X}$的每一行是一个样本，每一列是一种特征。对于特征集合$\mathbf{X}$，预测值$\hat{\mathbf{y}} \in \mathbb{R}^n$可以通过矩阵-向量乘法表示为：
$$
{\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b
$$

<p align="center">公式1.4</p>

该过程求和使用的是广播机制。给定训练数据特征$\mathbf{X}$和已知的标签$y$，线性回归的目标是找到一组权重向量$w$和偏置$b$。当给定从$\mathbf{X}$的同分布中取样的新样本特征时，找到的权重向量和偏置能够使得新样本预测标签的误差尽可能小。

模型优化即寻找最优$w$和$b$，我们还需要两个指标,

- 检测模型性能的度量方式
- 更新模型提高预测质量的方法

## 损失函数

在拟合之前，我们需要确定一个拟合程度的度量。损失函数能够量化目标的实际值和预测值之间的偏差。通常使用非负数作为损失，且数值越小损失越小，完美预测损失为0。以平方误差函数为例。当样本$i$的预测值为$\hat{y}^{(i)}$，真实标签为$y^{(i)}$时，平方误差定义为以下公式：
$$
l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.
$$

<p align="center">公式1.5</p>

常数$1/2$不会带来本质的区别，表现为对损失函数求导后常系数为1。由于平方误差函数的二次方项，预测值和真值之间的差距会进一步放大损失。为了度量模型在整个数据集上的质量，我们需要计算在训练集$n$个样本上的损失均值。


$$
L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.
$$

<p align="center">公式1.6</p>

在公式1.6中，$\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)$该项为权重和特征的矩阵乘积+偏置项减去真值即为误差值。除以$n$即为损失均值。

在训练模型时，我们希望有一组参数$\mathbf{w}^*, b^*$，这组参数能在最小化所有训练样本上的总损失，如下：
$$
\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).
$$

<p align="center">公式1.7</p>

## 随机梯度下降

在大部分深度学习问题中，是无法得到像线性回归一样的解析解的。但仍可有效的训练模型。在这里用到了一种名为梯度下降的算法。这种方法几乎可以优化所有深度学习模型。它通过不断地在损失函数递减的方向上更新参数来降低误差。

梯度下降最简单的用法是计算损失函数关于模型参数的导数。在实际执行的会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们挺长会在每次需要计算更新的时候随机抽取部分样本，这种变体较重小批量随机梯度下降算法。

在每次迭代中，首先随机抽样一个小批量$\mathcal{B}$，它由固定数量的训练样本组成。然后计算小批量平均损失关于模型参数的导数(各变量的偏导求和)。最后梯度乘以一个预先确定的正数$\eta$(Lr)，并从当前的参数值中减掉。公式如下：
$$
(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).
$$

<p align="center">公式1.8</p>

算法如下：

- 初始化模型参数的值，如随机初始化
- 从数据集中随机抽取小批量样本并且在负梯度的方向上更新参数，并不断迭代这一步骤，对于平方损失，如下：

$$
\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}
$$

<p align="center">公式1.9</p>

公式1.9中$\mathbf{w}$和$\mathbf{x}$都是向量。$|\mathcal{B}|$表示批样本数。$\eta$表示学习率。

在训练若干次之后，我们保存下模型的参数估计值，即$\hat{\mathbf{w}}, \hat{b}$。但是即使函数确实是线性且无噪声，这些估计值也不会使得损失函数真正达到最小值。因为算法会使得损失向最小值缓慢收敛，但是却不能在有限步数之内非常精确地达到最小值。

## 线性回归的实现

### 数据生成

我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 我们将使用低维数据，这样可以很容易地将其可视化。 在下面的代码中，我们生成一个包含1000个样本的数据集，每个样本包含从标准正态分布中采样的2个特征。我们的合成数据集是一个矩阵$\mathbf{X}\in \mathbb{R}^{1000 \times 2}$。

使用线性模型$\mathbf{w} = [2, -3.4]^\top$，$b = 4.2$和噪声$\epsilon$生成数据及标签。
$$
\mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon.
$$

<p align="center">公式1.10</p>

```Python
def synthetic_data(w, b, num_examples):  #@save
    """生成 y = Xw + b + 噪声。"""
    # 在给定的均值和标准差的离散分布中取随机数
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

输出对应的features, labels的shape为`torch.Size([1000, 2]) torch.Size([1000, 1])`。

### 批数据处理

```Python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

```Python
batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
```

### 初始化模型参数

```Python
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。有了这个梯度，我们就可以向减小损失的方向更新每个参数。

### 定义模型

```Python
def linreg(X, w, b):
    """线性回归模型。"""
    return torch.matmul(X, w) + b
```

对于线性模型，我们只需要特征$\mathbf{X}$和权重$\mathbf{w}$的矩阵-向量乘法加上偏置$b$，注意$\mathbf{X}\mathbf{w}$是一个向量，而$b$是一个标量，根据广播机制，标量会加到向量的每一个分量上。

### 定义损失函数

```Python
def squared_loss(y_hat, y):
    """均方损失。"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

因为要更新模型。需要计算损失函数的梯度，所以我们应该先定义损失函数

## 定义优化算法

在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。接下来，朝着减少损失的方向更新我们的参数。 下面的函数实现小批量随机梯度下降更新。该函数接受模型参数集合、学习速率和批量大小作为输入。每一步更新的大小由学习速率`lr`决定。 因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（`batch_size`）来归一化步长，这样步长大小就不会取决于我们对批量大小的选择。

```Python
def sgd(params, lr, batch_size):
    """小批量随机梯度下降。"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

### 训练

在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。最后，我们调用优化算法`sgd`来更新模型参数。
</script></section></div></div><script src="https://cdn.jsdelivr.net/npm/reveal.js@latest/dist/reveal.js"></script><script src="https://cdn.jsdelivr.net/npm/reveal.js@latest/plugin/zoom/zoom.js"></script><script src="https://cdn.jsdelivr.net/npm/reveal.js@latest/plugin/search/search.js"></script><script src="https://cdn.jsdelivr.net/npm/reveal.js@latest/plugin/notes/notes.js"></script><script src="https://cdn.jsdelivr.net/npm/reveal.js@latest/plugin/math/math.js"></script><script>Reveal.initialize({
  mouseWheel: false,
  transition: 'slide',
  transitionSpeed: 'default',
  parallaxBackgroundImage: '',
  parallaxBackgroundSize: '',
  parallaxBackgroundHorizontal: '',
  parallaxBackgroundVertical: '',
  autoSlide: false,
  loop: false,
  controlsLayout: 'bottom-right', // Determines where controls appear, "edges" or "bottom-right"
  controlsBackArrows: 'faded',
  progress: true,
  showNotes: false,
  autoPlayMedia: false,
  backgroundTransition: 'fade',
  markdown: {
    smartypants: true
  },
  plugins: [
  // Search
  RevealSearch,
  // Speaker notes
  RevealNotes,
  // Zoom in and out with Alt+click
  RevealZoom,
  // MathJax
  RevealMath
  ],
  dependencies: [
    { src: 'https://cdn.jsdelivr.net/npm/reveal.js@latest/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    // Syntax highlight for <code> elements
    { src: 'https://cdn.jsdelivr.net/npm/reveal.js@latest/plugin/highlight/highlight.js', async: true, callback: function() {
      // issue #218
      setTimeout(function () {
        hljs.initHighlighting();
      }, 0)
    } },
  ]
});</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"live2d-widget-model-tororo"},"display":{"superSample":2,"width":210,"height":420,"position":"left","hOffset":0,"vOffset":-20},"log":false,"tagMode":false});</script></body></html>